\PassOptionsToPackage{table}{xcolor}

\documentclass[oneside]{ausarbeitung}

\bibliography{references}

\usepackage{tikz}
\usepackage{listings}
\lstset{numbers=left, numberstyle=\tiny, numbersep=5pt}
\lstset{language=C}
\lstset{ % ... whatever was already there ...
        literate=% ... any other literates already there ...
                 {!}{!}1
                 {?}{?}1
                 {:}{:}1
}

\usepackage{dirtree}

\usepackage{xcolor}

\definecolor{lightblue}{RGB}{117, 200, 255}
\definecolor{lightgreen}{RGB}{84, 255, 129}

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

% syntax definition for glsl
%
% author: Sebastian Schäfer
% contact: 23@numb3r23.net

\lstdefinelanguage{GLSL}
{
	sensitive=true,
	alsoletter={\#},
	morekeywords=[1]{
		attribute, const, uniform, varying,
		layout, centroid, flat, smooth,
		noperspective, break, continue, do,
		for, while, switch, case, default, if,
		else, in, out, inout, float, int, void,
		bool, true, false, invariant, discard,
		return, mat2, mat3, mat4, mat2x2, mat2x3,
		mat2x4, mat3x2, mat3x3, mat3x4, mat4x2,
		mat4x3, mat4x4, vec2, vec3, vec4, ivec2,
		ivec3, ivec4, bvec2, bvec3, bvec4, uint,
		uvec2, uvec3, uvec4, lowp, mediump, highp,
		precision, sampler1D, sampler2D, sampler3D,
		samplerCube, sampler1DShadow,
		sampler2DShadow, samplerCubeShadow,
		sampler1DArray, sampler2DArray,
		sampler1DArrayShadow, sampler2DArrayShadow,
		isampler1D, isampler2D, isampler3D,
		isamplerCube, isampler1DArray,
		isampler2DArray, usampler1D, usampler2D,
		usampler3D, usamplerCube, usampler1DArray,
		usampler2DArray, sampler2DRect,
		sampler2DRectShadow, isampler2DRect,
		usampler2DRect, samplerBuffer,
		isamplerBuffer, usamplerBuffer, sampler2DMS,
		isampler2DMS, usampler2DMS,
		sampler2DMSArray, isampler2DMSArray,
		usampler2DMSArray, struct
	},
	morekeywords=[2]{
		radians,degrees,sin,cos,tan,asin,acos,atan,
		atan,sinh,cosh,tanh,asinh,acosh,atanh,pow,
		exp,log,exp2,log2,sqrt,inversesqrt,abs,sign,
		floor,trunc,round,roundEven,ceil,fract,mod,modf,
		min,max,clamp,mix,step,smoothstep,isnan,isinf,
		floatBitsToInt,floatBitsToUint,intBitsToFloat,
		uintBitsToFloat,length,distance,dot,cross,
		normalize,faceforward,reflect,refract,
		matrixCompMult,outerProduct,transpose,
		determinant,inverse,lessThan,lessThanEqual,
		greaterThan,greaterThanEqual,equal,notEqual,
		any,all,not,textureSize,texture,textureProj,
		textureLod,textureOffset,texelFetch,
		texelFetchOffset,textureProjOffset,
		textureLodOffset,textureProjLod,
		textureProjLodOffset,textureGrad,
		textureGradOffset,textureProjGrad,
		textureProjGradOffset,texture1D,texture1DProj,
		texture1DProjLod,texture2D,texture2DProj,
		texture2DLod,texture2DProjLod,texture3D,
		texture3DProj,texture3DLod,texture3DProjLod,
		textureCube,textureCubeLod,shadow1D,shadow2D,
		shadow1DProj,shadow2DProj,shadow1DLod,
		shadow2DLod,shadow1DProjLod,shadow2DProjLod,
		dFdx,dFdy,fwidth,noise1,noise2,noise3,noise4,
		EmitVertex,EndPrimitive
	},
	morekeywords=[3]{
		gl_VertexID,gl_InstanceID,gl_Position,
		gl_PointSize,gl_ClipDistance,gl_PerVertex,
		gl_Layer,gl_ClipVertex,gl_FragCoord,
		gl_FrontFacing,gl_ClipDistance,gl_FragColor,
		gl_FragData,gl_MaxDrawBuffers,gl_FragDepth,
		gl_PointCoord,gl_PrimitiveID,
		gl_MaxVertexAttribs,gl_MaxVertexUniformComponents,
		gl_MaxVaryingFloats,gl_MaxVaryingComponents,
		gl_MaxVertexOutputComponents,
		gl_MaxGeometryInputComponents,
		gl_MaxGeometryOutputComponents,
		gl_MaxFragmentInputComponents,
		gl_MaxVertexTextureImageUnits,
		gl_MaxCombinedTextureImageUnits,
		gl_MaxTextureImageUnits,
		gl_MaxFragmentUniformComponents,
		gl_MaxDrawBuffers,gl_MaxClipDistances,
		gl_MaxGeometryTextureImageUnits,
		gl_MaxGeometryOutputVertices,
		gl_MaxGeometryOutputVertices,
		gl_MaxGeometryTotalOutputComponents,
		gl_MaxGeometryUniformComponents,
		gl_MaxGeometryVaryingComponents,gl_DepthRange,
		\#version,core
	},
	morecomment=[l]{//},
	morecomment=[s]{/*}{*/},
	%morecomment=[l][keywordstyle4]{\#},
}


\newcommand*{\captionsource}[2]{%
  \caption[{#1}]{%
    #1%
    \\\hspace{\linewidth}%
    \textbf{Quelle:} #2%
  }%
}

\newcommand*{\quotize}[1]{\glqq #1\grqq}
\hypersetup{colorlinks=false,allcolors=black}

% ----------------------------------------------------------------------

\begin{document}

\selectlanguage{ngerman}

%--- Art der Arbeit
% Erlaubte Werte:
% Praxissemesterbericht, Projektbericht, Bachelorarbeit oder                                % Masterarbeit
\doctype{Bachelorarbeit}

%--- Studiengang:
\depname{Medieninformatik}

\title{WebGPU}

\author{Laurin Agostini}
\matrikelnr{60526}

\examinerA{Prof.~Dr.~Winfried~Bantel}
\examinerB{Prof.~Dr.~Carsten~Lecon}
\date{XX. Juni 2020}

%--- Titelseite Anzeigen
\maketitle
\cleardoublepage

%---
\pagenumbering{roman}
\setcounter{page}{1}


%--- Eidesstattliche Erklärung anzeigen
\makeaffirmation
\cleardoublepage

%---
\chapter*{Kurzfassung}
\addcontentsline{toc}{chapter}{Kurzfassung}
In dieser Bachelorarbeit geht es um die neuartige Grafik-API WebGPU, die einen Nachfolger zu WebGL darstellt. Mit WebGPU ist es möglich, detaillierte 3D-Szenen, aufwendige Simulationen und XXX in Echtzeit direkt im Webbrowser zu berechnen und darzustellen. 

%-----------------------------------------------------------------------
\cleardoublepage
\addcontentsline{toc}{chapter}{Inhaltsverzeichnis}
\tableofcontents

%---
\addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
\listoffigures

%---
\addcontentsline{toc}{chapter}{Tabellenverzeichnis}
\listoftables

\addcontentsline{toc}{chapter}{Listings}
\lstlistoflistings

%---
\chapter*{Abkürzungsverzeichnis}
\addcontentsline{toc}{chapter}{Abkürzungsverzeichnis}
\begin{acronym}[JSON]  % Längstes Kürzel in der nachfolgenden
                       % Liste um die Breite der Spalte für die
                       % Abkürzungen zu bestimmen.

%% Eintrag: \acro{Referenzname}[Kürzel]{Langform}
%% Im Text wird die Abkürzung dann mit \ac{Referenzname} benutzt.
\acro{json}[JSON]{JavaScript Object Notation}
\acro{CPU}[CPU]{Central Processing Unit (dt. Hauptprozessor)}
\acro{GPU}[GPU]{Graphics Processing Unit (dt. Grafikprozessor)}
\acro{SMT}[SMT]{Simultaneous Multithreading}
\acro{PCIe}[PCIe]{Peripheral Component Interconnect Express}
\acro{API}[API]{Application Programming Interface (dt. Programmierschnittstelle)}
\acro{WSL}[WSL]{Windows Subsystem for Linux}
\acro{GLSL}[GLSL]{OpenGL Shading Language}
\acro{HLSL}[HLSL]{High Level Shading Language}
\acro{PBR}[PBR]{Physically Based Rendering}
\acro{AMD}[AMD]{Advanced Micro Devices, Inc}
\end{acronym}
%---
\cleardoublepage
\pagenumbering{arabic}
\setcounter{page}{1}

% ----------------------------------------------------------------------
\chapter{Einleitung}
\label{cha:einleitung}

\section{Motivation}
\label{sec:motivation}

\section{Problemstellung und -abgrenzung}
\label{sec:problemstellung}

\section{Ziel der Arbeit}
\label{sec:ziel}

\section{Vorgehen}
\label{sec:vorgehen}


% ---
\chapter{Grundlagen}
\label{cha:grundlagen}

\section{GPU}
\label{sec:GPU}

\subsection{Aufgaben und Unterschied zur CPU}
\label{sub:GPU_tasks}
Wie der Name, \textit{\textbf{Grafik}prozessor} oder \textit{\textbf{graphics} processing unit}, schon andeutet, ist der Primärzweck einer \ac{GPU} die Erstellung und Manipulation von Bildinhalten und anschließender Ausgabe dieser an ein Anzeigegerät. Während eine \ac{CPU} generell darauf ausgelegt ist, ein weites Spektrum von Aufgaben sequenziell bearbeiten zu können, ist eine \ac{GPU} darauf spezialisiert möglichst viele einfache Aufgaben, insbesondere Fließkommaoperationen, parallel zu bearbeiten. Diesen Unterschied sieht man direkt, wenn man die Leistungsdaten aktueller \ac{CPU}s und \ac{GPU}s vergleicht:

\begin{table}
\begin{center}
\begin{tabular}{ |l|r|r| }
    \hline
    Name & Anzahl Kerne & Basis-/Turbotakt (MHz) \\
    \hline
    \multicolumn{3}{|c|}{High-end} \\
    \hline
    \rowcolor{lightblue}
    Intel® Core™ i9-9900K \cite{intel:i9_9900k} & 8 & 3.600 / 5.000 \\
    \rowcolor{lightgreen}
    NVIDIA GeForce RTX 2080Ti \cite{nvidia:rtx_2080ti} & 4.352 & 1.350 / 1.545 \\
    \hline
    \multicolumn{3}{|c|}{Low-end} \\
    \hline
    \rowcolor{lightblue}
    Intel® Core™ i3-9100 \cite{intel:i3_9100} & 4 & 3.600 / 4.200 \\
    \rowcolor{lightgreen}
    NVIDIA GeForce GTX 1650 \cite{nvidia:gtx_1650} & 896 & 1.485 / 1.665 \\
    \hline
\end{tabular}
Legende: \colorbox{lightblue}{\ac{CPU}} und \colorbox{lightgreen}{\ac{GPU}}
\end{center}
\caption{Vergleich aktueller CPUs und GPUs}
\end{table}

Hier sieht man sofort, dass \ac{GPU}s eine um mehrere Größenordnungen höhere Anzahl von Kernen haben, welche allerdings mit einer niedrigeren Taktrate laufen. Zwar konnte in den letzten Jahren ein starker Anstieg von Prozessorkernen in Endbenutzer-\ac{CPU}s beobachtet werden, doch auch eine explizite Workstation-\ac{CPU}, wie ein AMD Ryzen™ Threadripper™ 3990X \cite{amd:threadripper_3990x} mit 64 physischen und durch \ac{SMT} 128 logischen Prozessorkernen ist weit von den Kernzahlen einer aktuellen Einstiegs-\ac{GPU} entfernt. Wie schon angemerkt, kann man die Prozessorkerne von \ac{CPU}s und \ac{GPU}s nicht direkt vergleichen, jedoch zeigen sie deutlich die Spezialisierung der \ac{GPU}s auf parallele Verarbeitung mit einem erhöhtem Datendurchsatz. 

Zu den klassischen Aufgaben einer \ac{GPU} gehören Grafik-, Rechen-, Medien- und Displayfunktionalitäten. Durch die fortlaufende Entwicklung von fest in die Hardware programmierte Abläufe zu frei programmierbaren Anwendungen (ähnlich zu einer \ac{CPU}), werden jedoch immer mehr Anwendungen möglich, welche als \textbf{GPUCompute} zusammengefasst werden. Dies führte auch dazu, dass man in den letzten Jahren einen enormen Anstieg von \ac{GPU}s in Rechenzentren zum Beschleunigen von Anwendungen wie zum Beispiel \textit{machine learning} oder \textit{crypto mining}, beobachten konnte. Da sich \hyperref[cha:webgpu]{WebGPU} jedoch eher auf den klassischen Aufgabenbereich der \ac{GPU} bezieht, und dafür eine moderne Schnittstelle bereitstellt, wird sich im folgenden primär auf diese bezogen.

\subsection{Integrierte und dedizierte GPUs}
\label{sub:GPU_dedicated_integrated}
Mittlerweile haben die meisten Endbenutzer-\ac{CPU}s eine integrierte \ac{GPU} (oft auch \textbf{iGPU} genannt) um die üblichen Multimediaaufgaben zu beschleunigen und Last von der \ac{CPU} zu nehmen. Integrierte \ac{GPU}s sind aber für aufwendige Berechnungen wie zum Beispiel für grafikintensive Anwendungen nicht ausreichend. Dafür werden dedizierte Grafikkarten benötigt, welche der \ac{GPU} dedizierten (daher der Name) Arbeitsspeicher und oft auch eine aktive Kühlung bereitstellen. Grafikkarten werden heutzutage meist über \ac{PCIe} angeschlossen.

\begin{figure}
    \includegraphics[width=\textwidth]{images/1024px-Zotac_Gaming_GTX_2080_ti.jpg}
    \caption{Beispiel einer Grafikkarte (Zotax Gaming 2080 ti) \cite{2080_ti_graphics_card}}
    \label{fig:2080_ti_graphics_card}
\end{figure}

\section{Rendering-Pipeline \cite[Vgl.][The Graphics Rendering Pipeline]{real_time_rendering}}
\label{sec:render_pipeline}
\begin{quote}
The main function of the [graphics rendering] pipeline is to generate, or \textit{render}, a two-dimensional image, given a virtual camera, three-dimensional objects, light sources, and more. The rendering pipeline is thus the underlying tool for real-time rendering.

-- Real-Time Rendering \cite[S. 11]{real_time_rendering}
\end{quote}

\subsection{Architektur}
Grundsätzlich besteht die Rendering-Pipeline aus vier Abschnitten, die mehr oder weniger frei programmierbar sind (siehe Abbildung \ref{fig:render_pipeline}). Generell läuft der Abschnitt \textit{Anwendung} auf der \ac{CPU} (kann aber auch teilweise mithilfe von \ac{GPU}-Compute auf der \ac{GPU} implementiert werden) und beschreibt die Logik der Anwendung. Die drei folgenden Abschnitte \textit{Geometrieverarbeitung}, \textit{Rasterung} und \textit{Pixelverarbeitung} laufen alle auf der \ac{GPU}, wobei die \textit{Geometrie-} und \textit{Pixelverarbeitung} hier frei mit \textit{Shadern} programmierbar sind und die \textit{Rasterung} nur über Parameter konfiguriert werden kann.

Ursprünglich wurde der Begriff \textit{Shader} für Programme benutzt, die zur Farb- und Helligkeitsbestimmung einer 3D-Szene beigetragen haben. Da diese Aufgaben sich dann als Spezialität der \ac{GPU} herausgestellt haben, wurde der Begriff \textit{Shader} immer weiter gefasst und bezeichnet heute grundsätzlich ein \ac{GPU}-Programm.

\begin{figure}
    \includegraphics[width=\textwidth]{images/render_pipeline.png}
    \caption{Grundsätzlicher Aufbau der Rendering-Pipeline, unterteilt in die vier Abschnitte \textit{Anwendung}, \textit{Geometrieverarbeitung}, \textit{Rasterung} und \textit{Pixelverarbeitung}}
    \label{fig:render_pipeline}
\end{figure}

\subsection{Anwendung}
Der Abschnitt \textit{Anwendung} kann nicht konkret beschrieben werden, da er sich, wie der Name schon sagt, von Anwendung zu Anwendung unterscheidet. Jedoch kann man grundsätzlich sagen, dass hier die Logik der Anwendung stattfindet, wie zum Beispiel:
\begin{itemize}
\item{Benutzereingaben verarbeiten und auswerten}
\item{Physikberechnungen zwischen den Objekten}
\item{Ressourcen von der Festplatte laden}
\end{itemize}
Damit bereitet der Abschnitt \textit{Anwendung} die Daten für die restlichen Abschnitte vor und entscheidet, was dafür in Betrachtung gezogen wird (zum Beispiel nur Objekte in Blickrichtung der virtuellen Kamera).

\subsection{Geometrieverarbeitung}
\label{sub:geometry_processing}
Die \textit{Geometrieverarbeitung} ist dafür zuständig, die Geometriedaten (\textit{vertices} (Eckpunkte)) für die Rasterung vorzubereiten. Dafür müssen die \textit{vertices} meist zuerst von ihrer lokalen Position (innerhalb des Objektes) in die Position innerhalb des Sichtfeldes der virtuellen Kamera umgewandelt werden. 

\begin{figure}
    \includegraphics[width=\textwidth]{images/mvp_matrix.png}
    \caption{Transformation von Objekt bezogenen Vertexdaten in den \textit{world space}, dann in den \textit{view space} und schlussendlich in den \textit{clip space}. Die finale Umwandlung vom \textit{clip space} in den \textit{screen space} passiert dabei automatisch und verschiebt den Ursprung von der Mitte in entweder die ober oder untere (abhängig von der verwendeten Grafik-API) linke Ecke. Quelle: \url{https://learnopengl.com/img/getting-started/coordinate\_systems.png} \cite{learnopengl:coordinate_systems}}
    \label{fig:mvp_matrix}
\end{figure}

Dies passiert normalerweise mithilfe mehrerer Matrizen (siehe Abbildung \ref{fig:mvp_matrix}). Dabei wandelt die \textit{Model}-Matrix die lokalen Vertexdaten in das globale (\textit{world}) Koordinatensystem um. Die \textit{View}-Matrix wandelt dann die transformierten Daten wiederum in das lokale System der virtuellen Kamera um. Schlussendlich projiziert die \textit{Projection}-Matrix die Vertexdaten aus dem dreidimensionalen Raum auf die Bildebene. 

Die \textit{Model}-Matrix ist dabei abhängig von der Transformation des jeweiligen Objektes im \textit{world space}. Aus der Transformation der Kamera ergibt sich die \textit{View}-Matrix und die \textit{Projection}-Matrix wird aus den speziellen Eigenschaften der Kamera (Sichtfeld und \textit{near-}/\textit{far-plane} bei einer perspektivischen Projektion) gebildet. Manchmal werden die einzelnen Matrizen \textit{Model}, \textit{View} und \textit{Projection} auch zur sogenannten \textit{ModelViewProjection}-Matrix zusammengefasst um die Anzahl der Vektor-Matrix-Multiplikationen zu verringern.

Der Abschnitt Geometrieverarbeitung ist bis auf die finale Umwandlung vom \textit{clip space} in den \textit{screen space} frei programmierbar. Dabei wird ein zugewiesener Vertex-\textit{Shader} für jeden Vertex aufgerufen. Da dieser Abschnitt aber auf der \ac{GPU} läuft, kann man die Geometrieverarbeitung aber nicht mit einer üblichen Programmiersprache für \ac{CPU}s implementieren, sondern muss eine Programmiersprache für \textit{Shader}, wie zum Beispiel GLSL verwenden. Die Syntax von \ac{GLSL} ist dabei stark an der von C angelehnt. 

In Listing \ref{lst:vertex_shader} sieht man die Implementierung der Geometrieverarbeitung in der, bei dieser Arbeit entstandenen, \textbf{spider}-Engine. Dabei werden die Matrizen \textit{Model}, \textit{View} und \textit{Projection} einzeln dem Shader übergeben und sind für alle Shader-Ausführungen (für dieses Objekt) konstant. Jedoch unterscheiden sich bei jeder Ausführung die Daten des jeweiligen Vertex, für den der \textit{Shader }ausgeführt wird. Diese spezifischen Daten bekommt der Shader in den 4 Eingangsparametern ab Zeile 14 übergeben:
\begin{itemize}
\item{\textit{inPosition}: die Position des Vertex innerhalb des Objektes}
\item{\textit{inTexCoords}: die Texturkoordinaten des Vertex, zur späteren Bestimmung von Materialeigenschaften in der Pixelverarbeitung}
\item{\textit{inNormal}: die Normale des Vertex, meist aus den Normalen der angrenzenden Polygonflächen berechnet}
\item{\textit{inTangent}: die Tangente des Vertex, wird zur Anwendung von \textit{normal maps} in der Pixelverarbeitung benötigt}
\end{itemize}

Die 4 Ausgangsparameter ab Zeile 19 werden dann später, nach der Interpolation bei der Rasterung, in der Pixelverarbeitung benutzt. Wobei der Prefix \textit{frag} hier für \textit{fragment} steht, was eine alternative Bezeichnung für einen Pixel ist.
\begin{itemize}
\item{\textit{fragPosWorld}: die Position des Pixels im \textit{world space}}
\item{\textit{fragTexCoords}: die Texturkoordinaten des Pixels}
\item{\textit{fragNormal}: die Normale des Pixels}
\item{\textit{fragTangent}: die Tangente des Pixels}
\end{itemize}

\textit{gl\_Position} ist ein von \ac{GLSL} reservierter Ausgansparameter in dem die berechnete Vertexposition, zur weiteren Nutzung in folgenden Pipeline-Abschnitten, im \textit{clip space} gespeichert werden soll. Die lokale Vertexposition wird dabei zuerst vom lokalen System des Objektes mithilfe der \textit{Model}-Matrix in den \textit{world space} transformiert.  Diese Position wird dann einerseits direkt in \textit{fragPosWorld} abgespeichert und anderseits mithilfe der \textit{View}- und \textit{Projection}-Matrizen in den \textit{clip space} transformiert und in \textit{gl\_Position} gespeichert. Der Wert für \textit{fragTexCoords} wird direkt von \textit{inTexCoords} übernommen, da er unabhängig vom jeweiligen Koordinatensystem ist. Da \textit{inNormal} und \textit{inTangent} jeweils normalisierte Vektoren (also Richtungen) darstellen, können diese nicht mit der normalen \textit{Model}-Matrix in den \textit{world space} transformiert werden. Dazu muss erst die Transponierte der inversen \textit{Model}-Matrix gebildet werden. Mit dieser können dann \textit{fragNormal} und \textit{fragTangent} jeweils von \textit{inNormal} und \textit{inTangent} berechnet werden.

\begin{minipage}{\textwidth}
\begin{lstlisting}[language=GLSL, label={lst:vertex_shader}, caption={Beispiel eines Vertex Shaders in \ac{GLSL}, welcher so in der \textbf{spider}-Engine verwendet wird}]
#version 450
#extension GL_ARB_separate_shader_objects : enable

layout(set = 0, binding = 0) uniform Model {
    mat4 model;
};

layout(set = 0, binding = 1) uniform Camera {
    mat4 view;
    mat4 proj;
    vec3 pos;
} cam;

layout(location = 0) in vec3 inPosition;
layout(location = 1) in vec2 inTexCoords;
layout(location = 2) in vec3 inNormal;
layout(location = 3) in vec3 inTangent;

layout(location = 0) out vec3 fragPosWorld;
layout(location = 1) out vec2 fragTexCoords;
layout(location = 2) out vec3 fragNormal;
layout(location = 3) out vec3 fragTangent;

void main() {
    vec3 pos_world = vec3(model * vec4(inPosition, 1.0));
    gl_Position = cam.proj * cam.view * vec4(pos_world, 1.0);
    fragPosWorld = pos_world;
    fragTexCoords = inTexCoords;
    mat3 to_world = mat3(transpose(inverse(model)));
    fragNormal = to_world * inNormal;
    fragTangent = to_world * inTangent;
}
\end{lstlisting}
\end{minipage}
\subsection{Rasterung}
\label{sub:rasterizing}
Nach der \textit{Geometrieverarbeitung} sind nun alle Vertices des Polygons (im Folgenden zur Vereinfachung als Beispiel ein Dreieck) in den \textit{screen space} transformiert worden. Falls die Vertices im Bild liegen, d.h sie haben sowohl im vertikalen, als auch horizontalen Bereich des \textit{screen space} Werte zwischen 0 und 1, müssen nun die Pixel bestimmt werden, die in dem Dreieck liegen. Bei Polygonen die teilweise innerhalb und teilweise außerhalb des sichtbaren Bereichs liegen, wird das Polygon automatisch so zerstückelt, dass nur die sichtbaren Teile berücksichtigt werden (siehe Abbildung \ref{fig:screen_space}).

\begin{figure}
    \includegraphics[width=\textwidth]{images/screen_space_rasterizing.png}
    \caption{Zwei Dreiecke \textbf{T1} und \textbf{T2} im \textit{screen space}. \textbf{T1} wird hierbei komplett gerastert und \textbf{T2} erst so zerteilt, dass nur der sichtbare Bereich gerastert werden muss.}
    \label{fig:screen_space}
\end{figure}

Die \textit{Rasterung} ist hierbei fest in der Hardware einer \ac{GPU} verbaut und kann nur per Parameter konfiguriert werden. Dazu zählen zum Beispiel:

\begin{itemize}
\item{Art der zu rasternden Primitive: Punkte, Linien oder Dreiecke}
\item{Wie soll die Vorderseite eines Dreiecks bestimmt werden? Vertices sind entweder im oder gegen den Uhrzeigersinn angeordnet}
\item{Welche Seiten eines Dreieckes sollen berücksichtigt werden? Nur die Vorderseite, nur die Rückseite oder beide?}
\end{itemize}

Nun wird für jeden Pixel, der im Polygon liegt, die Vertexeigenschaften, d.h. die Ausgansparameter der Geometrieverarbeitung, zwischen den Vertices des Polygons interpoliert (siehe Abbildung \ref{fig:barycentric_color}). Für jeden dieser Pixel wird dann mit den interpolierten Eigenschaften als Eingangsparameter der \textit{Pixel-Shader}, oder auch \textit{Fragment-Shader}, zum finalen Abschnitt der Pixelverarbeitung aufgerufen. 

\begin{figure}
    \includegraphics[scale=1]{images/barycentriccolor.png}
    \caption{Interpolation von Vertexdaten innerhalb eines Dreiecks am Beispiel der Farbinformation der Vertices mithilfe von baryzentrischen Koordinaten \cite{scratchapixel:barycentric_color}}
    \label{fig:barycentric_color}
\end{figure}

\subsection{Pixelverarbeitung}
In der \textit{Pixelverarbeitung} wird der finale Farbwert eines Pixels berechnet im \textit{Framebuffer} gespeichert. Der \textit{Framebuffer} besteht dabei aus einem (oder auch mehreren) \textit{color buffer}, was im Prinzip ein zwei dimensionales Array (in Größe der Pixelanzahl: Breite * Höhe) für Farbwerte ist. Dazu kommt ein gleich großer \textit{depth buffer} der für jeden Pixel einen Tiefenwert (Distanz zur Kamera) abspeichert. Mit dem \textit{depth buffer} kann somit pro Pixel verglichen werden, ob der aktuell zu bearbeitende Punkt näher an der Kamera (Farbwert des Punktes wird berechnet und überschreibt den Tiefenwert) oder weiter weg ist (Punkt wird nicht weiter beachtet).

Der \textit{depth buffer} bietet hierbei den Vorteil, dass die Polygone nicht von \quotize{hinten (fern) nach vorne (nah)} gezeichnet werden müssen. Dadurch, dass jeder Punkt nur mit dem aktuell der Kamera am nächsten verglichen werden muss, können die Polygone in beliebiger Reihenfolge bearbeitet werden. Dies gilt allerdings nur für opake, nicht transparente, Objekte. Bei teil-transparenten Objekte muss trotzdem noch auf die Zeichenreihenfolge geachtet werden.

Falls der gerasterte Pixel erfolgreich mit dem aktuellen Tiefenwert verglichen wurde, d.h. er ist näher an der Kamera wird für ihn der \textit{Pixel-Shader}, im folgenden als \textit{Fragment-Shader} bezeichnet, aufgerufen. Als Beispiel kann hier der \textit{Fragment-Shader} für \ac{PBR} in der \textbf{spider}-Engine gesehen werden (siehe Anhang \ref{appendix:b}). Der  \textit{Fragment-Shader} berechnet dabei aus den interpolierten Vertexdaten und anderen Eingaben (zum Beispiel zusätzliche Daten wie Texturen) einen Farb- und Transparenzwert. Auch kann hier der Pixel noch verworfen werden, wenn zum Beispiel ein Transparenzwert von \textbf{0} (voll-transparent) berechnet wurde.

Die Pixelverarbeitung kann man also wiederum in drei Teilabschnitte unterteilen: 
\begin{itemize}
\item[1)]{Vergleichen mit dem aktuellen Tiefenwert aus dem \textit{depth buffer}}
\item[2)]{Berechnen des Farbwertes}
\item[3)]{Zusammenfügen mit dem aktuell im \textit{color buffer} gespeicherten Farbwert}
\end{itemize} 
Dabei ist hier nur der Teilabschnitt 2) frei per \textit{Shader} programmierbar. Teilabschnitte 1) und 3) können ähnlich wie im Abschnitt Rasterung (\ref{sub:rasterizing}) mit Parametern konfiguriert werden.

\section{Grafik-APIs \cite[Vgl.][The Evolution of Programmable Shading and APIs]{real_time_rendering}}
\textit{Der Begriff \quotize{\ac{GPU}} wurde erstmals 1999 von NVIDIA eingeführt, jedoch werden im folgenden der Einfachheit halber alle vorherigen Chips mit ähnlicher Funktion unter dem Begriff \quotize{\ac{GPU}} zusammengefasst.}

\begin{figure}
    \includegraphics[width=\textwidth]{images/api_timeline.png}
    \caption{Zeitlinie mit wichtigen Grafik-API- und Hardwareerscheinungen \cite[S. 38]{real_time_rendering}}
    \label{fig:barycentric_color}
\end{figure}

Analog zu den Programmiersprachen für \ac{CPU}s, wurden auch Ideen zur Programmierung von \ac{GPU}s, entwickelt bevor es die Hardware unterstützt hat. So wurden frei programmierbare \textit{Shader} wurde erstmals in Cooks \textit{Shade Trees} \cite{cook:shade_trees} erwähnt. Daraufhin wurde in den späten 1980er Jahren die RenderMan Shading Language \cite{pixar:rsl} entwickelt, welche auch heute noch in der Filmproduktion eingesetzt wird. Zu diesem Zeitpunkt waren \ac{GPU}s aber nichts weiter wie fest in die Hardware implementierte Bildspeicher mit einfachsten Funktionen \cite[Vgl.][]{gpu_history}.

Die erste \ac{GPU} zur Beschleunigung von 3D-Szenen, die 3dfx Voodoo \cite{wikipedia:3dfx_voodoo}, hatte die Rendering-Pipeline direkt in die Hardware implementiert. Auch die NVIDIA GeForce 256 \cite{wikipedia:geforce256}, die erste \ac{GPU}, die auch tatsächlich so bezeichnet wurde, konnte nicht programmiert werden. Dafür konnte die integrierte Rendering-Pipeline zumindest konfiguriert werden.

Im Jahre 2001 ist mit der NVIDIA Geforce 3 \cite{wikipedia:geforce3} die erste teilweise programmierbare \ac{GPU} erschienen. Durch die Grafik-APIs Microsoft DirectX 8.0 und das nicht-proprietäre OpenGL konnte hier die \textit{Vertex-Shader} mit einer Assembly-artigen Sprache programmiert werden. Mit DirectX 8.0 wurde auch das Konzept des \textit{Shader Model (SM)} \cite[Vgl.][Shader model comparison]{wikipedia:hlsl} eingeführt, welches spezifiziert welche Eigenschaften (zum Beispiel, wie viele Register dem Shader-Programm zur Verfügung stehen), die jeweilige Hardware erfüllen muss. Zwar unterstütze DirectX 8.0 auch programmierbare \textit{Pixel-Shader} (in OpenGL \textit{Fragment-Shader} genannt), jedoch waren diese noch sehr eingeschränkt im Funktionsumfang.

Dabei funktionieren \ac{GPU}s bis heute so, dass das gleiche Shader-Programm gleichzeitig auf mehrere Eingabedaten mit der selben Struktur, zum Beispiel Vertexdaten bei einem \textit{Vertex-Shader}, angewandt wird. Dadurch konnten in den Shader-Programmen für die ersten \ac{GPU}s noch keine Verzweigungen benutzt werden, da diese ja potentiell zu unterschiedliche Anweisungen für die jeweiligen Daten führen. Dies wurde von Shader-Programmierern dadurch umgangen, dass einfach alle Anweisungen aller potentiellen Verzweigungen nacheinander ausgeführt wurden und danach die nicht gebrauchten Ergebnisse verworfen wurden. Das führte zwar zu \quotize{nutzlosen} Berechnungen, jedoch war dies durch die Parallelität insgesamt immer noch schneller, als die Daten sequentiell zu verarbeiten.

Mit dem Erscheinen von DirectX 9.0 mit Shader Model 2.0 im Jahre 2002, hatten Programmierer endlich die volle Kontrolle über sowohl \textit{Vertex-} als auch \textit{Fragment-Shader}. Für den offenen Standard OpenGL wurde diese Möglichkeit relativ zeitgleich durch \textit{extensions} ermöglicht. Außerdem konnten die Shader nun in einer C ähnlichen Sprache geschrieben werden: \ac{HLSL} \cite{wikipedia:hlsl} für DirectX und \ac{GLSL} \cite{wikipedia:hlsl} für OpenGL. Mit der Einführung des Shader Model 3.0 im Jahre 2004 konnte nun endlich auch ein dynamischer Programmablauf mit Verzweigungen im Shader benutzt werden. Mit der Einführung der Spielekonsole Nintendo Wii Ende 2006 wurde auch eine der letzten \ac{GPU}s mit fester Rendering-Pipeline eingeführt. 

Ebenfalls wurde Ende 2006 DirectX 10.0 und Shader Model 4.0 eingeführt, welche eine neue Art von Shadern (\textit{Geometry-Shader}) und die Vereinheitlichung des Shader-Designs mit sich brachte. Mit DirectX 11.0 und Shader Model 5.0 im Jahre 2009 wurden schließlich unter anderem \textit{Compute-Shader} eingeführt. Die Version von OpenGL mit ähnlichem Funktionsumfang ist dabei 4.3.

Bis dahin war das Paradigma für Grafik-APIs dem Entwickler so viel Arbeit wie möglich abzunehmen und zum Beispiel im Treiber Eingaben zu validieren und Ressourcen (zum Beispiel Texturen und Buffer) automatisch zu verwalten. Dies änderte sich 2013 mit der Einführung von Mantle durch den \ac{CPU}- und \ac{GPU}- Hersteller \ac{AMD} als Alternative zu Direct3D und OpenGL. Die Idee hinter der API, die in Zusammenarbeit mit dem Videospielentwickler DICE entwickelt wurde, war es, den Overhead im Grafiktreiber so viel wie möglich zu reduzieren und diese Kontrolle dem Entwickler zurückzugeben. Microsoft hat die Ideen hinter Mantle aufgenommen und 2015 mit DirectX 12.0 eine DirectX Version dieses neuen Paradigmas veröffentlicht. Wichtig ist hier, dass DirectX 12 keinen Fokus auf neuen Funktionen hat, sondern nur die Verwendung der API radikal geändert hat. Bei der Einführung von DirectX 12.0 hatte es die gleichen Funktionalitäten wie das damals aktuelle DirectX 11.3. Auch Apple hat 2014 mit Metal ihre eigene \quotize{low-overhead} API veröffentlicht. Diese debütierte zuerst auf den mobilen Geräten wie iPhone und iPad, war jedoch ein Jahr später auch auf Macs verfügbar. Währenddessen hatte AMD ihre Arbeit zu Mantle an das für OpenGL zuständige Konsortium Khronos Group gestiftet. Diese führte 2016 mit Vulkan ihre Version einer \quotize{low-overhead} API ein. Ähnlich wie bei den DirectX-Versionen 11 und 12, hat Vulkan hier OpenGL nicht abgelöst, sondern stellt nur eine Alternative dar. Mit Vulkan wurde auch eine Zwischensprache (engl. \textit{intermediate language}) für Shader-Programme, ähnlich wie LLVM IR, mit dem Namen SPIR-V eingeführt, die es dem Entwickler erlaubt Shader vor zu kompilieren.

Für mobile Geräte gibt es OpenGL ES (\quotize{Embedded Systems}) welches 2003 als eine im Funktionsumfang verringerte Version von OpenGL 1.3 veröffentlicht wurde. OpenGL ES 1.0 beschrieb noch eine feste Rendering-Pipeline, welche aber durch Version 2.0 im Jahr 2007 die Möglichkeiten zur Shader-Programmierung erhielt. OpenGL ES 3.0 wurde schließlich 2012 eingeführt und brachte viele bekannte Funktionen der \quotize{großen Brüder} OpenGL und DirectX mit.

Die Grafik-API für Webbrowser WebGL basiert wiederum auf OpenGL ES, wurde 2011 veröffentlicht und ist mit Version 2.0 von OpenGL ES im Funktionsumfang äquivalent. WebGL 2.0 wurde 2017 eingeführt und nutzt OpenGL ES 3.0.


\chapter{WebGPU}
\label{cha:webgpu}

\section{Entstehung \cite[Vgl.][]{wikipedia:webgpu}}
Da WebGL auf dem alten API-Paradigma von OpenGL und DirectX vor Version 12 beruht, wurden Rufe laut, dass es auch eine \quotize{moderne} Grafik-API für Webbrowser geben sollte, um deren Vorteile auch im Web nutzen zu können.

Die ersten Vorstöße dazu gab es als Google Mitarbeiter bei einer Präsentation Mitte 2016 für die WebGL Arbeitsgruppe über die grundlegenden Ideen und Prinzipien für eine moderne Grafik-API fürs Web, damals noch als \quotize{WebGL Next} bezeichnet, sprachen. Diese Ideen wurden dann Anfang 2017 von der Khronos Group aufgegriffen und in einem Meeting hat ein Google Team dann ihren mit \quotize{NXT} betitelten Prototypen präsentiert, welcher im Webbrowser Chromium lief und sich an Konzepten von Vulkan, DirectX 12 und Metal bediente. Teams von Apple und Mozilla hatten außerdem Prototypen für Safari und Servo (die Browser Engine für Firefox) gebaut, welche sich an Apples Metal API orientiert haben. 

Kurze Zeit später wurde die W3C Gruppe \quotize{GPU for the Web} gegründet und \quotize{WebGPU} als Arbeitstitel für die neue API beschlossen \cite{golem:webgpu}. Nachdem die meisten oberflächlichen Probleme im Standardisierungsprozess Mitte 2018 gelöst waren, gab Googles Chrome Team bekannt, dass sie den zukünftigen \textbf{WebGPU} in ihren Webbrowser implementieren möchten.

\section{Beschreibung}
\begin{minipage} {\textwidth}
\begin{quote}
\quotize{The mission of the GPU on the Web Community Group is to provide an interface between the Web Platform and modern 3D graphics and computation capabilities present in native system platforms. The goal is to design a new Web API that exposes these modern technologies in a performant, powerful and safe manner. It should work with existing platform APIs such as Direct3D 12 from Microsoft, Metal from Apple, and Vulkan from the Khronos Group. This API will also expose the generic computational facilities available in today's GPUs to the Web, and investigate shader languages to produce a cross-platform solution.}

-- GPU for the Web Community Group \cite{w3:community_gpu}
\end{quote}
\end{minipage}

\textbf{WebGPU} soll hierbei ähnlich wie bei Vulkan und OpenGL, und DirectX 12 und 11 nicht WebGL direkt ablösen, sondern erstmal nur eine Alternative bilden.

\section{WGSL}
\subsection{Übersicht}
Mit WGSL \cite{w3:wgsl} soll auch \textbf{WebGPU} eine eigene Shader-Sprache bekommen. Bisherige Implementierungen unterstützen jedoch bisher nur SPIR-V. Dadurch konnte der Verfasser leider keine praktischen Erfahrungen bei der Verwendung von WGSL machen. Die \textbf{spider}-Engine, die diese Arbeit begleitet hat, verwendet daher GLSL-Shader, die zu SPIR-V kompiliert werden. Trotzdem werden im Folgenden die Grundprinzipien und Beispiele zu WGSL besprochen, da es in Zukunft wohl ein wichtiger Bestandteil von \textbf{WebGPU} sein wird.

\subsection{Kritik an WGSL}
WGSL soll \quotize{trivial von und zu SPIR-V konvertierbar} \cite[Vgl.][Goals]{w3:wgsl} sein, was unter anderem die Frage aufwirft, warum dann WGSL überhaupt benötigt wird \cite[Vgl.][]{github:wgsl_terrible}. Durch die Nähe zu der Zwischensprache SPIR-V ist die Syntax von WGSL für Entwickler, die mit einer bisherigen Shader-Sprache vertraut sind, nicht sehr üblich. Auch gibt es nicht die aus den meisten Programmiersprachen bekannten Schleifenstrukturen, sondern nur eine \texttt{loop}-Anweisung, aber nicht direkt \texttt{for}, \texttt{while} oder \texttt{do-while} \cite[Vgl.][]{github:wgsl_loop}. In Zukunft soll nur noch WGSL als Shader-Sprache akzeptiert werden. Dazu müssten die bei vielen Entwicklern schon bestehenden GLSL- oder HLSL-Shader erst konvertiert werden. Dies würde in etwas so aussehen:

\texttt{GLSL/HLSL -> SPIR-V -> WGSL -> *WebGPU* -> SPIR-V/DXIL/MSL}

Für die Umwandlung von GLSL/HLSL zu SPIR-V gibt es bereits zahlreiche Tools, welche produktiv getestet und eingesetzt wurden, wie zum Beispiel, das in diesem Projekt genutzte \textit{glslc}, welches in der Shader-Werkzeugsammlung \textit{shaderc} \cite{shaderc} enthalten ist. Der abschließende Schritt, bei dem \textbf{WebGPU} den WGSL-Shader-Code wieder in \texttt{SPIR-V/DXIL/MSL} umwandeln muss, ist nötig, da WebGPU auf den jeweiligen nativen Grafik-APIs (Vulkan, DirectX 12, Metal) aufbaut, welche keinen WGSL-Shader-Code nutzen können.

Die Ursache für die Entscheidung für WGSL und gegen SPIR-V als primäres Shader-Code-Format, liegt wohl einerseits an der für das Web übliche Praxis, von Menschen lesbare Datenformate zu verwenden (kein Binärcode). Anderseits scheint es wohl rechtliche Spannungen zwischen Apple und der für SPIR-V verantwortlichen Khronos Group zu geben \cite[Vgl.][]{hacker_news:apple_khronos}.

\subsection{Datentypen}

\subsubsection{Skalare Typen}
\label{sub:scalars}
\begin{table}[htb]
\begin{center}
\begin{tabular}{ |l|l|l|l| }
    \hline
    \textbf{Typ} & \textbf{Beschreibung} & \textbf{GLSL} & \textbf{HLSL} \\
    \hline
    \multicolumn{4}{|l|}{Boolean} \\
    \hline
    \texttt{bool} & Entweder \texttt{true} oder \texttt{false} & \texttt{bool} & \texttt{bool} \\
    \hline
    \multicolumn{4}{|l|}{Numerisch} \\
    \hline
    \texttt{i32} & 32-bit Integer mit Vorzeichen & \texttt{int} & \texttt{int} \\
    \hline
    \texttt{u32} & 32-bit Integer ohne Vorzeichen & \texttt{uint} & \texttt{uint} \\
    \hline
    \texttt{f32} & 32-bit Fließkommazahl nach IEEE 754 & \texttt{float} & \texttt{float} \\
    \hline
    \textit{-} & 64-bit Fließkommazahl nach IEEE 754 & \texttt{double} & \texttt{double} \\
    \hline
\end{tabular}
\end{center}
\caption{Skalare Typen in WGSL mit den äquivalenten Typen in GLSL und HLSL}
\label{tab:scalars}
\end{table}

\subsubsection{Mehrkomponenten Typen}
\paragraph{Vektortypen} \

Vektortypen werden in WGSL mit 

\texttt{vec\textit{N}<\textit{T}>}

deklariert, wobei \textit{N} für die Anzahl der Komponenten und \textit{T} für den Datentyp steht. \textit{N} muss dabei in \{2, 3, 4\} und \textit{T} muss einer der skalaren Typen aus Abschnitt \ref{sub:scalars} sein.

Das äquivalente Gegenstück hierzu in GLSL wäre 

\texttt{\textit{T}vec\textit{N}}

wobei \textit{T} hier als Abkürzung eines skalaren Typen fungiert (b = \texttt{bool}, i = \texttt{int}, u = \texttt{uint}, d = \texttt{double}). Wenn \textit{T} weggelassen wird, wird der skalare Typ \texttt{float} verwendet, da dies der mit Abstand meist verwendete skalare Typ in der Shader-Programmierung ist. \textit{N} muss hier ebenfalls in \{2, 3, 4\} sein.

Bei HLSL werden Vektortypen wie folgt deklariert:

\texttt{\textit{T}\texttt{N}}

Wobei \textit{T} hier einer der skalaren Typen sein muss und \textit{N} entweder in \{1, 2, 3, 4\} oder gar nicht vorkommen darf. Das Weglassen von \textit{N} oder N = 1 resultieren beide in dem skalaren Typen \textit{T}.

So würde ein 3-komponentiger Vektor vom Typ \texttt{f32} bzw. \texttt{float} in den jeweiligen Sprachen so aussehen:
\begin{itemize}
 \item WGSL: \texttt{vec3<f32>}
 \item GLSL: \texttt{vec3}
 \item HLSL: \texttt{float3}
\end{itemize}

\paragraph{Matrixtypen} \

Analog zu Vektortypen werden Matrixtypen in WGSL mit

\texttt{mat\textit{N}x\textit{M}<\textit{T}>}

deklariert, wobei \textit{N} hier für die Anzahl der Spalten und \textit{M} für die Anzahl der Zeilen steht. \textit{N} und \textit{M} müssen wiederum beide in \{2, 3, 4\} sein. \textit{T} ist ein skalarer Typ aus Abschnitt \ref{sub:scalars}.

In GLSL gibt es dafür

\texttt{mat\textit{N}x\textit{M}}

und als Abkürzung für eine Matrix mit quadratischer Form (Anzahl Spalten = Anzahl Zeilen)

\texttt{mat\textit{N}}

Matrixkomponenten sind immer vom Typ \texttt{float} oder \texttt{double}. Eine Matrix mit doppelter Genauigkeit wird mit \texttt{dmat\textit{N}x\textit{M}} bzw \texttt{dmat\textit{N}} deklariert.

In HLSL werden die Matrixtypen ähnlich wie die Vektortypen deklariert:

\texttt{\textit{T}\texttt{N}x\texttt{M}}

wobei hier wie bei GLSL für \textit{T} jeder der skalare Typen benutzt werden kann. \textit{N} steht für die Anzahl der Spalten und \textit{M} für die Anzahl der Zeilen und beide müssen jeweils in \{1, 2, 3, 4\} sein.

Eine quadratische Matrix mit 3 Spalten und 3 Zeilen vom Typ \texttt{f32} bzw. \texttt{float} wird also jeweils so erstellt:
\begin{itemize}
 \item WGSL: \texttt{mat3x3<f32>}
 \item GLSL: \texttt{mat3x3} bzw. \texttt{mat3}
 \item HLSL: \texttt{float3x3}
\end{itemize}

Um wiederum eine Matrix mit 3 Spalten und 2 Zeilen vom Typ \texttt{i32} bzw. \texttt{int} zu erstellen werden folgende Deklarationen benötigt:

\begin{itemize}
 \item WGSL: \texttt{mat3x2<i32>}
 \item GLSL: \textit{-}
 \item HLSL: \texttt{int3x2}
\end{itemize}

\paragraph{Swizzling} \

Um auf die Komponenten eines Mehrkomponenten Typen zuzugreifen haben diese vordefinierte Felder. So hat ein 4-komponentiger Vektor zum Beispiel die Felder \texttt{x, y, z, w}. Da aber 4-komponentige Vektor auch häufig für Farbwerte benutzt werden, gibt es zusätzlich noch die Felder \texttt{r, g, b, a}, wobei dies aber nur andere Namen für die gleichen Komponenten sind: \texttt{x = r, y = g, z = b, w = a}. GLSL hat zusätzlich noch das Namensset \texttt{s, t, p, q}, das im Zusammenhang mit Texturkoordinaten benutzt wird.

Eine Besonderheit in vielen Shader-Sprachen ist es, dass diese Felder frei kombiniert werden können um die einzelnen Komponenten umzustrukturieren, was \textit{swizzling} \cite{wikipedia:swizzling} genannt wird (siehe Listing \ref{lst:swizzling}).

\begin{minipage}{\textwidth}
\begin{lstlisting}[language=GLSL, label={lst:swizzling}, caption={\quotize{Swizzling} von Komponenten in GLSL}]
vec4 some_vec = vec4(1.0, 2.0, 3.0, 4.0);
// Get the second component
float val = some_vec.y;
// Get the fourth as the first, and the first as the second component of a new vec2
vec2 other_vec = some_vec.ar; 
// Write to the first, third and second component
some_vec.xzy = vec3(0.0, some_vec.xx);
// But you can't mix name sets
vec3 faulty_vec = some_vec.xrr;
\end{lstlisting}
\end{minipage}

Dabei wird es in WGSL nicht möglich sein, in mehrere Komponenten gleichzeitig zu schreiben, also einen \quotize{swizzled} Ausdruck auf der linken Seite einer Zuweisung zu haben \cite[Vgl.][Typed Storage]{w3:wgsl}.

\section{Der aktuelle Stand}
\subsection{Spezifikation}
Die Spezifikation \cite{w3:webgpu} der \textbf{WebGPU}-\ac{API} ist zum Zeitpunkt des Verfassens dieses Abschnittes (09.06.2020) noch nicht abgeschlossen und wurde auch während dem Verlauf der Arbeit mehrmals erweitert und angepasst.

\subsection{Implementierung}
\textit{Der aktuelle Stand der Implementierung in Webbrowsern kann unter \cite{webgpu:implementation_status} eingesehen werden.}

\subsubsection{Chromium \cite{chromium} / Dawn \cite{google:dawn}}
\textit{Dawn} ist eine Open-Source \textbf{WebGPU}-Implementierung, die in dem Open-Source Webbrowser Chromium verwendet wird. Unter anderem basieren die bekannten Webbrowser \textit{Google Chrome}, \textit{Opera} und seit neustem auch \textit{Microsoft Edge} auf Chromium. 

Die \quotize{native} Implementierung von \textit{Dawn} benutzt dafür die Grafik-APIs der jeweils ausführenden Plattform:

\begin{itemize}
\item{\textit{D3D12} auf Windows 10}
\item{\textit{Metal} auf macOS und iOS}
\item{\textit{Vulkan} auf Windows, Linux und Google eigenen Betriebssystemen (ChromeOS, Android, Fuchsia)}
\item{\textit{OpenGL} wo verfügbar}
\end{itemize}

Das heißt, dass die \textbf{WebGPU}-Befehle hier im Hintergrund die jeweiligen Befehle der nativen Grafik-API aufrufen. Dies hat den Vorteil, dass \textbf{WebGPU} so keinen eigenen Treiber braucht, um mit der \ac{GPU} zu kommunizieren. Da die generelle \textbf{WebGPU}-API-Struktur auch ähnlich zu der Struktur der nativen APIs (bis auf OpenGL) ist, bedeutet dies auch keinen großen Mehraufwand bei der Implementierung und kann sogar teilweise von Codegeneratoren bewerkstelligt werden.

\subsubsection{Firefox Nightly \cite{mozilla:firefox_nightly} / wgpu \cite{mozilla:wgpu}}
\textit{wgpu} ist ebenfalls eine Open-Source \textbf{WebGPU}-Implementierung in der Programmiersprache \textit{Rust}. \textit{wgpu} wird hierbei unter anderem im \textit{Mozilla Firefox}-Webbrowser verwendet und von der \textit{Mozilla Foundation} federführend entwickelt.
\section{Nutzen außerhalb von Grafik}

%---
\chapter{Implementierung einer WebGPU-Applikation}
\label{cha:implementierung}
\textit{Für das in diesem Kapitel beschriebene Projekt wurde Windows 10 mit Ubuntu 19.10 per \ac{WSL} \cite{microsoft:wsl} verwendet. Dabei wurden die Quellcodedateien in Windows erstellt und bearbeitet, aber in Ubuntu benutzt (kompilieren, linken usw.). Deshalb beziehen sich im Folgenden alle Angaben zur Installation oder Benutzung von Werkzeugen auf die jeweilige Linux-Version.}

\section{spider}
\label{sec:spider}
\textit{In diesem Abschnitt wird mit \quotize{der Benutzer}, ein Benutzer der \textbf{spider}-Engine verstanden, also eine Person, die mithilfe der Engine eine eigene Applikation entwickelt. \quotize{Der Benutzer} sollte hier als neutrale Form, die die weibliche, sowie die männliche Form beinhaltet, verstanden werden.}
\subsection{Überblick}
\label{sub:overview}
Mit der parallel zu dieser Arbeit entstandenen \textbf{spider}-Engine, lässt sich mit geringem Aufwand eine Webapplikation zur Darstellung von 3D-Szenen in C/C++ erstellen.
Das Grundgerüst dafür ist sehr minimal (siehe Listing \ref{lst:spider_base}).

\begin{minipage}{\textwidth}
\begin{lstlisting}[language=C, label={lst:spider_base}, caption={Grundgerüst einer Applikation mit der \textbf{spider}-Engine}]
#include "spider/spider.h"

// Create the initial state of your scene
void initApplication();
// Update your scene each frame
bool update(float delta_time_s);

int main() {
	const uint32_t surface_width = 1280;
	const uint32_t surface_height = 720;

	// Initialize the spider engine
	spInit(&(SPInit){
		.surface_size = {
			.width = surface_width,
			.height = surface_height
		},
		.update_func = update,
		.camera = {
			.pos = {0.0f, 2.0f, 5.0f},
			.look_at = {0.0f, 0.0f, 0.0f},
			.mode = SPCameraMode_LookAt,
			.fovy = glm_rad(60.0f),
			.aspect = (float)surface_width / (float) surface_height,
			.near = 0.1f,
		},
	});	
	
	initApplication();
	spStart();
	
	return 0;
}

void initApplication() {
	// Here you can create lights (only spotlights for now),
	// create custom meshes and materials 
	// or load them from glTF files (recommended)
}

bool update(float delta_time_s) {
	// Here you can update your created lights, objects and the main camera
	
	// Return false, if you want to quit the application
	return true;
}
\end{lstlisting}
\end{minipage}

Die \textbf{spider}-Engine ist darauf ausgelegt, mit möglichst wenig Code auf Seiten des Benutzers eine 3D-Applikation zu erstellen. So lässt sich in rund 150 Zeilen Code eine interaktive 3D-Szene mit einem UserInterface (dank integriertem Dear ImGui \cite{dear_imgui}) erstellen (siehe Abbildung \ref{fig:spider_example_sponza} und Anhang \ref{appendix:a}). Trotzdem ist die Funktionalität der \textbf{spider}-Engine sehr beschränkt, wenn man etwas anderes als das Darstellen von 3D-Modellen und eines einfachen UserInterfaces will. Jedoch ist der Verfasser überzeugt, dass es für einfache Prototypen ausreichend ist.

\begin{figure}
    \includegraphics[width=\textwidth]{images/state_of_work_20200608.png}
    \caption{Interaktive 3D-Szene mit UserInterface. Bildschirmaufnahme durch Verfasser.}
    \label{fig:spider_example_sponza}
\end{figure}

\subsection{Codestyle}
\label{sub:codestyle}
\textit{Für das Projekt wurde der C Standard C99 und der von \textbf{Emscripten} verwendete Compiler clang benutzt. Folgende Punkte sollten in allen größeren C-Compilern, die C99 unterstützen, problemlos funktionieren, wurden dort aber nicht getestet.}

Um einen einheitlichen Codestyle zu erhalten, hat der Verfasser sich auf folgende Punkte festgelegt:

\subsubsection{\textit{stdint.h} und \textit{stdbool.h}}
Im Projekt wurden die seit C99 verfügbaren Headerdateien \texttt{stdint.h} und \texttt{stdbool.h} benutzt, um einerseits mehr Kontrolle über den Speicherbedarf der jeweiligen Komponenten zu erlangen und anderseits mehr Kontext zu geben. So ist zum Beispiel bei einer Variable des Typen \texttt{uint8\_t} schnell klar, dass der Verfasser für die Benutzung der Variable nur einen relativ kleinen Bereich an positiven Ganzzahlen beabsichtigt hat. Auch der Typ \texttt{bool} ist im Vergleich zu einem \texttt{int} eindeutig im Bezug auf die Absicht des Verfassers. Interessanterweise wurde beim gesamten Projekt (außer bei der Kommunikation mit den verwendeten Bibliotheken) nur Ganzzahldatentypen ohne Vorzeichen benutzt (\texttt{uint8\_t}, \texttt{uint16\_t}, \texttt{uint32\_t} und \texttt{uint64\_t}).

\subsubsection{Prefixe}
Da es in C keine Namensbereiche, wie zum Beispiel in C++, gibt, werden alle Strukturen, Enums, Funktionen, Preprozessordirektiven und globale Variablen mit einem Prefix versehen (siehe Tabelle \ref{tab:prefixes}). Die Unterscheidung zwischen \textit{öffentlich} und \textit{privat} ist hierbei keine Unterscheidung auf Compilerlevel, sondern nur für den Benutzer, damit dieser erkennen kann, welche Funktionen und Strukturen benutzt werden sollten und welche nur intern genutzt werden.

\begin{table}
\begin{center}
\begin{tabular}{ |l|l|l| }
\hline
\textbf{Typ} & \textbf{Prefix} & \textbf{Beispiel} \\
\hline
\multicolumn{3}{|c|}{\quotize{öffentlich}} \\
\hline
Struktur & \texttt{SP} & \texttt{SPMesh} \\
\hline
Enum & \texttt{SP} & \texttt{SPKey} \\
\hline
Funktion & \texttt{sp} & \texttt{spInit} \\
\hline
Preprozessordirektive & \texttt{SP\_} & \texttt{SP\_INVALID\_ID} \\
\hline
globale Variable & \texttt{sp\_} & \textit{kein Beispiel} \\
\hline
\multicolumn{3}{|c|}{\quotize{privat}} \\
\hline
Struktur & \texttt{\_SP} & \texttt{\_SPRenderPipeline} \\
\hline
Enum & \texttt{\_SP} & \textit{kein Beispiel} \\
\hline
Funktion & \texttt{\_sp} & \texttt{\_spSetupPools} \\
\hline
Preprozessordirektive & \texttt{\_SP\_} & \texttt{\_SP\_MATERIAL\_POOL\_DEFAULT} \\
\hline
globale Variable & \texttt{\_sp\_} & \texttt{\_sp\_state} \\
\hline
\end{tabular}
\end{center}
\caption{Verwendete Prefixe}
\label{tab:prefixes}
\end{table}

\subsubsection{Definition von Strukturen}
Strukturen werden im Projekt wie in Listing \ref{lst:structs} definiert.

\begin{minipage}{\textwidth}
\begin{lstlisting}[language=C, label={lst:structs}, caption={Definition von Strukturen}]
typedef struct MyStructure {
	uint32_t x;
	struct {
		float width;
		float height;
	} size;
} MyStructure;
\end{lstlisting}
\end{minipage}

Dies hat den Vorteil, dass die so definierten Strukturen nicht immer ein vorhergehendes \texttt{struct} bei der Verwendung im Code benötigen. Zusätzlich wird direkt nach dem \texttt{struct} auch der Name der Struktur benötigt, damit diese weiterhin vorwärts deklariert werden kann \cite[vgl.][]{weissflog:structs}. 

Semantisch zusammengehörige Attribute innerhalb einer Struktur werden entweder in eine extra Struktur ausgelagert, oder als anonyme Struktur (siehe \texttt{size} im Beispiel) innerhalb der Struktur definiert. Dadurch kann über \texttt{my\_structure.size.width} auf das Attribut zugegriffen werden.

\subsubsection{\textit{Desc}-Argument}
In vielen Programmiersprachen ist es möglich, beim Aufrufen einer Funktion nur einen Teil der Argumente anzugeben, während die restlichen Argumente mit Standardwerten initialisiert werden. C unterstützt zwar keine Standardargumente, dafür aber ab C99 \textit{Bestimmte Initialisierer (engl. designated initializers)}, was bedeutet, dass man die Felder von Strukturen (und Arrays) mit ihrem jeweiligen Namen (oder Index) in beliebiger Reihenfolge innerhalb einer Initialisierungsanweisung angeben kann. Wenn mindestens ein Feld initialisiert wird, werden alle nicht initialisierten Felder mit \textbf{0} initialisiert. (Zur Verdeutlichung siehe Listing \ref{lst:designated_initializers}). 

\begin{minipage}{\textwidth}
\begin{lstlisting}[language=C, label={lst:designated_initializers}, caption={Beispiele zu bestimmten Initialisierern (Unter Verwendung der in Listing \ref{lst:structs} definierten Struktur \textit{MyStructure})}]
// No guarantee on initialization values
MyStructure my_structure;

// Initialization in field order 
// -> (x = 5, size.width = 3.0f, size.height = 4.0f)
MyStructure my_structure_order = {5, {3.0f, 4.0f}};
MyStructure my_structure_order2 = {5, 3.0f, 4.0f};

// Initialization in field order, rest is zero initialized
// -> (x = 5, size.width = 0.0f, size.height = 0.0f)
MyStructure my_structure_order_zero = {5};

// Initialization with field names
// -> (x = 5, size.width = 3.0f, size.height = 4.0f)
MyStructure my_structure_named = {.size.width = 3.0f, .x = 5, .size.height = 4.0f};
MyStructure my_structure_named2 = {.size = {.width = 3.0f, .height = 4.0f }, .x = 5};

// Initialization with field names, rest is zero initialized
// -> (x = 0, size.width = 3.0f, size.height = 0.0f)
MyStructure my_structure_named_zero = {.size.width = 3.0f};
MyStructure my_structure_named_zero2 = {.size = {.width = 3.0f}};

// Initialization of array fields
// -> (0, 3, 0, 0, 1)
uint32_t array[5] = {
	[1] = 3,
	[4] = 1
};
\end{lstlisting}
\end{minipage}

Im Projekt wird das bei Funktionen mit einer nicht trivialen Argumentliste verwendet. Dabei wird für die jeweilige Funktion eine Struktur mit dem Postfix \texttt{Desc} (für engl. \textit{descriptor}, analog zu den \textbf{WebGPU}-\textit{Descriptor}-Strukturen) definiert. Diese enthält alle benötigten Argumente (mit möglichst sinnvollem Standardwert \textbf{0}). Da es außerdem in C99 möglich ist, einen Zeiger auf ein temporäres Objekt zu erzeugen, hat die jeweilige Funktion nun als einziges Argument einen Zeiger auf ein konstantes Objekt der \texttt{Desc}-Struktur: \texttt{void myFunction(const MyFunctionDesc* desc);}. Nun kann die \texttt{Desc}-Struktur entweder vor dem Aufrufen der Funktion erschaffen und befüllt werden, oder direkt beim Funktionsaufruf initialisiert werden (siehe Listing \ref{lst:function_desc}).

\begin{minipage}{\textwidth}
\begin{lstlisting}[language=C, label={lst:function_desc}, caption={Verwendung einer \textit{Desc}-Struktur zum Übergeben von Argumenten an eine Funktion}]
typedef struct MyFunctionDesc {
	uint32_t values[5];
	const char* name;
} MyFunctionDesc;

void myFunction(const MyFunctionDesc* desc);

MyFunctionDesc my_desc;
my_desc.values[2] = 5;
my_desc.name = "Abc";
myFunction(&my_desc);

myFunction(&(MyFunctionDesc){
	.values = {
		[2] = 5,
	},
	.name = "Abc",
});
\end{lstlisting}
\end{minipage}

\subsubsection{\textit{handles} \cite[Vgl.][]{weissflog:handles}}
\label{subsub:handles}

Die \textbf{spider}-Engine hat das Grundprinzip, dass der Benutzer sich weder um die (De-)Allokation von Speicher, noch über die Referenzierungen zwischen Objekten, kümmern muss und somit den vollen Fokus auf die Erstellung der jeweiligen Applikation richten kann. Dabei soll auch die Speicherverwaltung von erschaffenen Objekten (zum Beispiel ein \texttt{SPMesh}) einzig der \textbf{spider}-Engine überlassen werden. Dabei soll die Engine auch die Möglichkeit haben, die jeweiligen Objekte intern zu verschieben und zu sortieren (bisher nicht benutzt) um den internen Zugriff zu optimieren. Daher kann die Engine nicht direkt einen Zeiger auf ein erschaffenes Objekt liefern, da dieser bei einer Änderung der internen Struktur ungültig wird.

Dafür gibt es für jede \textit{öffentliche} Struktur eine \textit{handle}-Struktur mit Postfix \textit{ID} (zum Beispiel für \texttt{SPMesh} -> \texttt{SPMeshID}) welche eine interne ID (\textit{uint32\_t id}) beinhaltet. Bisher wird die interne ID des \textit{handle} einfach direkt als Index in das interne Array zur Verwaltung der jeweiligen Objekte benutzt, jedoch könnte man hier noch ähnlich zu \cite{weissflog:handles} die unbenutzten Bits (bei zum Beispiel maximal $2^{16} = 65536$ gleichzeitig existierenden Objekten bleiben hier 16 von 32 Bits übrig) zur Versionierung des \textit{handle} verwenden. Um das referenzierte Objekt dann im Anwendungscode zu benutzen kann ein temporärer Zeiger erzeugt werden (siehe Listing \ref{lst:handle}). Dieser Zeiger sollte vom Benutzter nicht gespeichert werden und nur erzeugt werden, wenn das referenzierte Objekt wirklich verwendet (Attribute lesen oder schreiben) wird. Es gibt keine Garantie, dass der Zeiger im nächsten Update noch auf das gleiche Objekt zeigt, oder überhaupt gültig ist.

\begin{minipage}{\textwidth}
\begin{lstlisting}[language=C, label={lst:handle}, caption={Verwendung von \textit{handles}}]
typedef struct SPObject {
	float size;
} SPObject;

typedef struct SPObjectID {
	uint32_t id;
} SPObjectID;

// Store the object handle
SPObjectID object_id;

void init() {
	// Create object and store the handle
	object_id = spCreateObject();
}

void update() {
	// Later use the referenced object
	SPObject* object = spGetObject(object_id);
	// If handle is not valid, spGetObject returns a NULL pointer
	if(object) {
		object->size *= 2.0f;
	}
}
\end{lstlisting}
\end{minipage}

\section{emscripten \cite{emscripten}}
Augenscheinlich wurde das Projekt, das diese Arbeit begleitet hat, in C geschrieben. Da die entstandene Applikation jedoch später in einem Webbrowser laufen und dort die \textbf{WebGPU}-\ac{API} benutzen soll, \textbf{<TODO>}. Dazu wird das Werkzeug \textbf{Emscripten} verwendet. Mit \textbf{Emscripten} lässt sich (unter anderem) C-Quellcode mit Hilfe von LLVM \cite{llvm} zu JavaScript und WebAssembly \cite{wasm} kompilieren.

\subsection{Funktionsweise}
Um \textbf{Emscripten} benutzen zu können, muss als erstes das Emscripten SDK (emsdk) heruntergeladen und installiert werden \cite{emsdk}. Am einfachsten ist es hierbei, das SDK im gleichen Ordner zu platzieren, wie das Projekt, das es benutzen soll:
\dirtree{% 
.1 path\textbackslash.
	.2 to\textbackslash.
		.3 emsdk\textbackslash.
		.3 my\_project\textbackslash.
			.4 test.c.
}
Zum Beginn einer Terminal-Sitzung müssen noch die Umgebungsvariablen für \textbf{Emscripten} gesetzt werden (dies macht man am besten im \textit{path\textbackslash to\textbackslash emsdk\textbackslash}-Ordner):

\texttt{source ./emsdk\_env.sh}

Im Projektordner (\textit{path\textbackslash to\textbackslash my\_project\textbackslash}) kann nun die C-Quellcodedatei kompiliert werden:

\texttt{./emcc test.c -WASM=1 -o test.html}

\section{Benutzte WebGPU-Features}
Die benutzten Features der \ac{WebGPU}-API wurden nach mehreren Punkten ausgesucht:
\begin{itemize}
\item{Die Implementierung sollte sowohl in Chromium Canary als auch in Firefox Nightly verfügbar sein, um vergleich- und testbar zu sein}
\item{Es sollte einen konkreten Nutzen in der \textbf{spider}-Engine erfüllen, um das Projekt nicht unnötig kompliziert zu machen}
\end{itemize}


\section{Besonderheiten bei der Entwicklung einer GPU Applikation}
Dadurch, dass viele Befehle nicht auf der \ac{CPU}, sondern auf der \ac{GPU} ablaufen, sind traditionelle \textit{Debugger}, wie \textit{gdb}, nur begrenzt nützlich zum Finden von Fehlern im Programm. Damit kann man zwar immer noch den generellen Ablauf des Programms überprüfen, aber was genau nach dem Aufrufen einer \ac{API}-Funktion auf der \ac{GPU} geschieht, ist damit nicht einsehbar. Dafür gibt es verschiedene Grafik-Debugger, die praktisch alle nach dem gleichen Prinzip ablaufen. So muss meist das zu testende Programm über den Grafik-Debugger gestartet werden, damit dieser sich in das Programm einklinken kann. Im Gegensatz zu klassischen Debuggern muss das Programm dabei nicht als \textbf{Debug}-Version gebaut werden. Wenn das zu testende Programm nun erfolgreich gestartet wurde, hat man die Möglichkeit einen oder mehrere \textit{frames}(Einzelbilder) zu erfassen. Die jeweiligen \textit{frames} werden dann aufbereitet und man kann sich alle vom Programm getätigten \ac{API}-Aufrufe und alle \ac{GPU}-Ressourcen zum jeweiligen Zeitpunkt anschauen.

Durch die Besonderheit der Applikation im Bezug auf die Neuheit der \textbf{WebGPU}-\ac{API} und der Tatsache, dass die Applikation im Webbrowser läuft, konnten allerdings mehrere der Grafik-Debugger nicht sinnvoll verwendet werden. So hatten die oft benutzten Programme \textit{RenderDoc} \cite{renderdoc} und \textit{NVIDIA NSight Graphics} \cite{nvidia:nsight_graphics} Probleme damit, mit den benutzten Webbrowser-Versionen \textit{Chrome Canary}  \cite{google:chrome_canary} und \textit{Firefox Nightly} \cite{mozilla:firefox_nightly} eine Verbindung aufzubauen. \textit{RenderDoc} konnte dabei nur die \textit{Direct3D 11} Befehle zur Anzeige des finalen Bildes, aber nicht die eigentlichen \textit{Direct3D 12} Befehle zur Erstellung des Bildes, erfassen. Bei \textit{NVIDIA NSight} war ein Erfassen von \textit{frames} gar nicht möglich, da \textit{D3D11on12} \cite{microsoft:d3d11on12} (eine Softwareschicht um \textit{Direct3D 11}-Befehle auf \textit{Direct3D 12}-Befehlen zu übersetzen), das in \textit{Chrome} benutzt wird, gar nicht unterstüzt wird. Einzig mit \textit{PIX on Windows} \cite{microsoft:pix} war es möglich alle \ac{API}-Aufrufe und \ac{GPU}-Ressourcen  richtig anzuzeigen. Jedoch auch nicht als \textbf{WebGPU} Aufrufe, sondern als \textit{Direct3D 12} Aufrufe und nur wenn \textit{Chrome Canary} als Webbrowser benutzt wird. Da \textbf{WebGPU} aber, wie schon angesprochen, sehr ähnlich zu \textit{Direct3D 12} ist, ist dies kein großes Problem.

Zum korrekten Starten des \textit{Chrome Canary} Webbrowser durch \textit{PIX on Windows}, müssen noch zusätzliche Kommandozeilenparameter übergeben werden: 
\texttt{--no-sandbox --disable-gpu-sandbox --disable-gpu-watchdog --disable-direct-composition} 
und als API \textit{D3D12 (ignore D3D11)} ausgewählt werden (wie in Abbildung \ref{fig:pix_options} zu sehen). Mit \textbf{Launch} wird dann der richtig konfigurierte\textit{Chrome Canary} gestartet und man kann nun auf die zu testende Webseite (bei diesem Projekt zum Beispiel \textit{https://localhost:8080/release}) wechseln und in \textit{PIX on Windows} können nun mit einem Klick auf das Kamerasymbol \textit{frames} erfasst werden. Nach kurzer Zeit erscheinen dann die erfassten \textit{frames} in einer Liste darunter und können dann detailliert angeschaut werden (siehe Abbildung \ref{fig:pix_overview}).

\begin{figure}
    \includegraphics[width=\textwidth]{images/pix_options.png}
    \caption{Übersicht der Optionen zum Starten von \textit{Google Chrome Canary} in \textit{Microsoft PIX on Windows} \cite{microsoft:pix}. Bildschirmaufnahme durch Verfasser.}
    \label{fig:pix_options}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{images/pix_overview.png}
    \caption{Anzeige nach der Erfassung eines Einzelbildes in \textit{Microsoft PIX on Windows} \cite{microsoft:pix}. Bildschirmaufnahme durch Verfasser.}
    \label{fig:pix_overview}
\end{figure}


%---
\chapter{Evaluierung}
\label{cha:evaluation}


%---
\chapter{Zusammenfassung und Ausblick}
\label{cha:zusammenfassung}

\section{Erreichte Ergebnisse}
\label{sec:ergebnisse}


\section{Ausblick}
\label{sec:ausblick}

\subsection{Erweiterbarkeit der Ergebnisse}
\label{sub:erweiterbarkeit}

\subsection{Übertragbarkeit der Ergebnisse}
\label{sub:uebertragbarkeit}


%-----------------------------------------------------------------------
\addcontentsline{toc}{chapter}{Referenzen}
\printbibliography[title={Referenzen}]

\appendix

%---
\chapter{Anhang A}
\label{appendix:a}
Dies ist der Quellcode zu der in Abbildung \ref{fig:spider_example_sponza} gezeigten Beispielanwendung. Die Anwendung lädt dabei das relativ komplexe 3D-Modell \textbf{Sponza} \cite{sponza} aus einer glTF-Datei \cite{khronos:gltf}, lässt den/die Anwender*in sich mit Hilfe einer steuerbaren Kamera im Raum bewegen und durch das UserInterface verschiedene Parameter der Szene verändern.
\begin{lstlisting}[language=C, label={lst:full_example}, caption={Kompletter C99-Quellcode zur Erstellung einer interaktiven 3D-Szene mit der \textbf{spider}-Engine}]
#include "spider/spider.h"

static SPLightID spot_light_id;
static uint32_t last_mouse_pos_x = 0;
static uint32_t last_mouse_pos_y = 0;
static const uint32_t surface_width = 1280;
static const uint32_t surface_height = 720;
static vec3 cam_rot = {0.0f, 0.0f, 0.0f};
static vec4 forward = {0.0f, 0.0f, 1.0f, 0.0f};
static float sensitivity = 2.0f;
static float vertical_limit = 0.01f;

void init(void) {
    // Lights have to be created before materials right now 
    const vec3 light_pos = {0.0f, 5.0f, 0.5f};
    const vec3 light_look_at = {2.0f, 0.0f, 0.0f};
    vec3 light_direction = {-1.0, -1.0f, 0.2f};
    // (float*) cast to prevent compiler warning 'incompatible-pointer-types-discards-qualifiers'
    // cglm takes no const pointers as arguments, even if it doesn't mutate the vectors
    glm_vec3_sub((float*)light_look_at, (float*)light_pos, light_direction);
    glm_vec3_normalize(light_direction);

    spot_light_id = spCreateSpotLight(&(SPSpotLightDesc){
            .pos = {light_pos[0], light_pos[1], light_pos[2]},
            .range = 40.0f,
            .color = {.r = 255, .g = 255, .b = 255},
            .dir = {light_direction[0], light_direction[1], light_direction[2]},
            .fov = glm_rad(70.0f),
            .power = 20.0f,
            .shadow_casting = &(SPLightShadowCastDesc){
                .shadow_map_size = 2048,
            },
        }
    );
    SP_ASSERT(spot_light_id.id != SP_INVALID_ID);

    /*SPSceneNodeID sponza_node_id = */spLoadGltf("assets/gltf/Sponza/Sponza.gltf");
}

bool update(float delta_time_s) {
    static bool show_controls = true;
    igBegin("Controls", &show_controls, ImGuiWindowFlags_None);
        igText("Look:");
            igBulletText("Hold right mouse button and move mouse");
        igText("Move:");
            igBulletText("W: Forward");
            igBulletText("S: Back");
            igBulletText("A: Left");
            igBulletText("D: Right");
            igBulletText("Space: Up");
            igBulletText("LeftControl: Down");
    igEnd();

    static bool scene_control = true;
    igBegin("Scene control", &scene_control, ImGuiWindowFlags_None);
    
    SPCamera* cam = spGetActiveCamera();
    glm_vec4_normalize(forward);

    if(cam) {
        if(spGetMouseButtonPressed(SPMouseButton_Right)) {
            vec2 relative_delta = {
                ((float)spGetMousePositionX() - (float)last_mouse_pos_x) / (float) surface_width,
                ((float)spGetMousePositionY() - (float)last_mouse_pos_y) / (float) surface_height
            };
            float rotation_speed = sensitivity * M_PI;
            cam_rot[1] -= rotation_speed * relative_delta[0]; // horizontal
            cam_rot[0] += rotation_speed * relative_delta[1]; // vertical
            cam_rot[0] = glm_clamp(cam_rot[0], (-M_PI * 0.5f) + vertical_limit, (M_PI * 0.5f) - vertical_limit);
        }
        memcpy(forward, (vec4){0.0f, 0.0f, 1.0f, 0.0f}, sizeof(vec4));
        mat4 rot = GLM_MAT4_IDENTITY_INIT;
        glm_euler_zyx(cam_rot, rot);
        glm_mat4_mulv(rot, forward, forward);

        cam->dir[0] = forward[0];
        cam->dir[1] = forward[1];
        cam->dir[2] = forward[2];
        vec3 sideward = {
            -forward[2],
            0.0f,
            forward[0],
        };
        glm_vec3_normalize(sideward);
        const float walk_speed = 2.0f;
        const float forward_movement = walk_speed * delta_time_s * (-1.0f * spGetKeyPressed(SPKey_S) + spGetKeyPressed(SPKey_W));
        const float sideward_movement = walk_speed * delta_time_s * (-1.0f * spGetKeyPressed(SPKey_A) + spGetKeyPressed(SPKey_D));
        const float upward_movement = walk_speed * delta_time_s * (-1.0f * spGetKeyPressed(SPKey_ControlLeft) + spGetKeyPressed(SPKey_Space));
        cam->pos[0] += forward[0] * forward_movement + sideward[0] * sideward_movement;
        cam->pos[1] += forward[1] * forward_movement + upward_movement;
        cam->pos[2] += forward[2] * forward_movement + sideward[2] * sideward_movement;
        
        if(igCollapsingHeaderTreeNodeFlags("Camera", ImGuiTreeNodeFlags_None)) {
            igSliderFloat("Look sensitivity##cam", &sensitivity, 0.0f, 5.0f, "%.1f", 1.0f);
            igSliderFloat("Vertical look limit##cam", &vertical_limit, 0.0f, M_PI * 0.5f, "%.2f", 1.0f);
            igSliderFloat3("Position##cam", (float*)&cam->pos, -10.0f, 10.0f, "%.2f", 1.0f);
            igSliderFloat3("Rotation (Rad)##cam", (float*)&cam_rot, -M_PI, M_PI, "%.2f", 1.0f);
            igSliderFloat("Vertical field of view (Rad)##cam", &cam->fovy, 0.01f, M_PI, "%.2f", 1.0f);
        }
    }

    last_mouse_pos_x = spGetMousePositionX();
    last_mouse_pos_y = spGetMousePositionY();
    
    SPLight* spot_light = spGetLight(spot_light_id);
    if(spot_light) {
        if(igCollapsingHeaderTreeNodeFlags("Spotlight", ImGuiTreeNodeFlags_None)) {
            igSliderFloat3("Position##light", (float*)&spot_light->pos, -50.0f, 50.0f, "%.1f", 1.0f);
            igSliderFloat("Field of view##light", &spot_light->fov, 0.0f, M_PI, "%.2f", 1.0f);
            igSliderFloat("Power##light", &spot_light->power, 0.0f, 1000.0f, "%.0f", 1.0f);
            igSliderFloat("Range##light", &spot_light->range, 0.0f, 1000.0f, "%.0f", 1.0f);
        }
    }
    igEnd();

    // return false if you want to quit
    return true;
}

int main() {
    spInit(&(SPInitDesc){
        .surface_size = {
            .width = surface_width,
            .height = surface_height
        },
        .update_func = update,
        .camera = {
            .pos = {0.0f, 2.0f, 0.0f},
            .dir = {0.0f, 0.0f, 1.0f},
            .look_at = {0.0f, 0.0f, 0.0f},
            .mode = SPCameraMode_Direction,
            .fovy = glm_rad(60.0f),
            .aspect = (float)surface_width / (float) surface_height,
            .near = 0.1f,
        },
        .pools.capacities = {
            .meshes = 128,
            .materials = 64,
            .render_meshes = 256,
            .lights = 1,
            .scene_nodes = 1024,
        },
        .show_stats = true,
    });

    init();

    spStart();
    return 0;
}
\end{lstlisting}

%---
\chapter{Anhang B}
\label{appendix:b}

\begin{lstlisting}[language=glsl, label={lst:fragment_shader}, caption={Kompletter GLSL-Code des Fragment-Shaders der in der \textbf{spider}-Engine} für das \ac{PBR} benutzt wird]
#version 450
#extension GL_ARB_separate_shader_objects : enable
#extension  GL_EXT_samplerless_texture_functions : enable

layout(set = 0, binding = 1) uniform Camera {
    mat4 view;
    mat4 proj;
    vec3 pos;
} cam;

layout(set = 0, binding = 2) uniform Light {
    mat4 view;
    mat4 proj;
    vec4 pos3_range1;
    vec4 color3_type1; // type: 0 directional, 1 spot, 2 point
    vec4 dir3_fov1; // dir: for spot & dir, fov: for spot
    vec4 area2_power1_padding1; // area: for dir
} light;  // TODO: support more than 1 light

layout(set = 2, binding = 0) uniform texture2D albedo_tex;
layout(set = 2, binding = 1) uniform sampler albedo_sampler;
layout(set = 2, binding = 2) uniform texture2D normal_tex;
layout(set = 2, binding = 3) uniform sampler normal_sampler;
layout(set = 2, binding = 4) uniform texture2D ao_roughness_metallic_tex;
layout(set = 2, binding = 5) uniform sampler arm_sampler;
layout(set = 2, binding = 6) uniform texture2D shadow_map;
layout(set = 2, binding = 7) uniform sampler shadow_sampler;


layout(location = 0) in vec3 fragPosWorld;
layout(location = 1) in vec2 fragTexCoords;
layout(location = 2) in vec3 fragNormal;
layout(location = 3) in vec3 fragTangent;

layout(location = 0) out vec4 outColor;

const float PI = 3.14159265359;
const float gamma = 2.2;

const int sampleSize = 1;
const float scale = 1.5;

float getLightDepthOnPosSingle(vec2 coords_shadow_map) {
    return texture( sampler2D( shadow_map, shadow_sampler ), coords_shadow_map.xy ).r;
}

float getLightDepthOnPosSampled(vec2 coords_shadow_map) {
    float value = 0.0;
    ivec2 texDim = textureSize(shadow_map, 0);
    float dx = scale * 1.0 / float(texDim.x);
    float dy = scale * 1.0 / float(texDim.y);
    int count = 0;
    for(int y = -sampleSize; y <= sampleSize; ++y) {
        for(int x = -sampleSize; x <= sampleSize; ++x) {
            value += getLightDepthOnPosSingle(coords_shadow_map + vec2(x * dx, y * dy));
            count++;
        }
    }
    return value / count;
}

float DistributionGGX(vec3 N, vec3 H, float roughness) {
    float a      = roughness*roughness;
    float a2     = a*a;
    float NdotH  = max(dot(N, H), 0.0);
    float NdotH2 = NdotH*NdotH;
	
    float num   = a2;
    float denom = (NdotH2 * (a2 - 1.0) + 1.0);
    denom = PI * denom * denom;
	
    return num / denom;
}

float GeometrySchlickGGX(float NdotV, float roughness) {
    float r = (roughness + 1.0);
    float k = (r*r) / 8.0;

    float num   = NdotV;
    float denom = NdotV * (1.0 - k) + k;
	
    return num / denom;
}
float GeometrySmith(vec3 N, vec3 V, vec3 L, float roughness) {
    float NdotV = max(dot(N, V), 0.0);
    float NdotL = max(dot(N, L), 0.0);
    float ggx2  = GeometrySchlickGGX(NdotV, roughness);
    float ggx1  = GeometrySchlickGGX(NdotL, roughness);
	
    return ggx1 * ggx2;
}

vec3 fresnelSchlick(float cosTheta, vec3 F0) {
    return F0 + (1.0 - F0) * pow(1.0 - cosTheta, 5.0);
}

void main() {
    const vec4 albedo_all = texture(sampler2D(albedo_tex, albedo_sampler), fragTexCoords).rgba;
    const vec3 albedo = albedo_all.rgb;
    const float alpha = albedo_all.a;
    if(alpha == 0.0) {
        discard;
    }
    vec3 tangent_space_normal = normalize(texture(sampler2D(normal_tex, normal_sampler), fragTexCoords).xyz * vec3(2.0, 2.0, 1.0) - vec3(1.0, 1.0, 0.0));
    const vec3 ao_roughness_metallic = texture(sampler2D(ao_roughness_metallic_tex, arm_sampler), fragTexCoords).rgb;
    const float ao          = ao_roughness_metallic.r;
    const float roughness   = ao_roughness_metallic.g;
    const float metallic    = ao_roughness_metallic.b;

    vec3 Normal = normalize(fragNormal);
    vec3 Tangent = normalize(fragTangent);
    // enforce orthogonality
    Tangent = normalize(Tangent - dot(Tangent, Normal) * Normal);
    const vec3 Bitangent = cross(Tangent, Normal);
    mat3 TBN = mat3(Tangent, Bitangent, Normal);
    vec3 normal = normalize(TBN * tangent_space_normal);

    vec3 N = normalize(normal);
    vec3 V = normalize(cam.pos - fragPosWorld);
    vec3 F0 = vec3(0.04); 
    F0 = mix(F0, albedo, metallic);
	           
    // reflectance equation
    vec3 Lo = vec3(0.0);
    vec3 light_pos = light.pos3_range1.xyz;
    float distance    = length(light_pos - fragPosWorld);
    vec3 L = normalize(light_pos - fragPosWorld);
    float light_fov = 1.0 - (light.dir3_fov1.w / 3.14);
    float attenuation = light.area2_power1_padding1.z * (light_fov * light_fov);
    attenuation *= max(1.0 - distance / light.pos3_range1.w, 0.0);
    
    // For spot lights
    if(light.color3_type1.w == 1.0) {
        const float light_angle_rad = acos(dot(light.dir3_fov1.xyz, -L));
        attenuation *= pow(max(light.dir3_fov1.w * 0.5 - light_angle_rad, 0.0) / 3.14, 1.0 - (light_fov * light_fov));
    }
    // Shadow calculation
    const vec4 pos_shadow_map = light.proj * light.view * vec4(fragPosWorld, 1.0);
    vec4 pos_in_light_clip_space = pos_shadow_map / pos_shadow_map.w;
    pos_in_light_clip_space.xy = pos_in_light_clip_space.xy * 0.5 + 0.5; // [-1,1] to [0,1]
    pos_in_light_clip_space.y = 1.0 - pos_in_light_clip_space.y; // bottom-up to top-down
    // [0, 0] of pos_in_light_clip_space.xy should now be the top left corner and [1, 1] the bottom right --> texture space

    const float depth_bias = 0.00001; // Depth bias is not yet implemented in Chromium/dawn, so we have to "fake" it in the shader
    if(attenuation > 0.0 && pos_in_light_clip_space.z > 0.0 && pos_in_light_clip_space.z < 1.0) {
        const float light_depth_on_pos = getLightDepthOnPosSampled(pos_in_light_clip_space.xy);
        if(pos_in_light_clip_space.w > 0.0 && light_depth_on_pos - depth_bias > pos_in_light_clip_space.z) {
            attenuation = 0.0;
        }
    }
    if(attenuation > 0.0) {
        // calculate per-light radiance
        vec3 H = normalize(V + L);
        vec3 radiance     = light.color3_type1.rgb * attenuation;
        
        // cook-torrance brdf
        float NDF = DistributionGGX(N, H, roughness);
        float G   = GeometrySmith(N, V, L, roughness);
        vec3 F    = fresnelSchlick(max(dot(H, V), 0.0), F0);
        
        vec3 kS = F;
        vec3 kD = vec3(1.0) - kS;
        kD *= 1.0 - metallic;
        
        vec3 numerator    = NDF * G * F;
        float denominator = 4.0 * max(dot(N, V), 0.0) * max(dot(N, L), 0.0);
        vec3 specular     = numerator / max(denominator, 0.001);  
            
        // add to outgoing radiance Lo
        float NdotL = max(dot(N, L), 0.0);                
        Lo += (kD * albedo / PI + specular) * radiance * NdotL;
    }
  
    vec3 ambient = vec3(0.04) * albedo * ao;
    vec3 color = ambient + Lo;
	
    color = color / (color + vec3(1.0));
    color = pow(color, vec3(1.0/gamma));  
   
    outColor = vec4(color, alpha);
}
\end{lstlisting}


\end{document}